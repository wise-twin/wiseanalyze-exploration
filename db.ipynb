{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install tqdm\n",
    "%pip install psycopg2-binary\n",
    "%pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f5bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Sync errors\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'    # Detailed asserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid5, UUID\n",
    "import psycopg2\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "UUID_NAMESPACE = UUID(\"c87c53d6-4464-4018-b4c9-15718d354ec8\")\n",
    "UUID_NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04ec38",
   "metadata": {},
   "source": [
    "# ARIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bbe1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./accidents-tous-req10905.csv\", encoding=\"cp1252\", sep=\";\", skiprows=7)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44642c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_db(df : pd.DataFrame, trunc = None):    \n",
    "\n",
    "    if trunc is not None :\n",
    "        df = df.head(trunc)\n",
    "\n",
    "    def create_line(line : pd.Series):\n",
    "        address = \" \".join([str(line[\"Départment\"]), str(line[\"Commune\"])])\n",
    "        site_id = str(uuid5(UUID_NAMESPACE, address))\n",
    "        sites = {\n",
    "            \"site_id\" : site_id,\n",
    "            \"plant_name\": \"\",\n",
    "            \"address\": address,\n",
    "            \"latitude\": None,               # to fill later\n",
    "            \"longitude\": None,              # to fill later\n",
    "            \"country\": line[\"Pays\"],\n",
    "            \"industrial_activity\": line[\"Code NAF\"],\n",
    "        }\n",
    "\n",
    "        accident_key = \" \".join([str(line[\"Titre\"]), str(line[\"Date\"])])\n",
    "        accident_id = str(uuid5(UUID_NAMESPACE, accident_key))\n",
    "        accidents = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"site_id\": site_id,\n",
    "            \"title\": line[\"Titre\"],\n",
    "            \"source\": \"ARIA\",\n",
    "            \"source_id\": str(line[\"Numéro ARIA\"]),\n",
    "            \"accident_date\": line[\"Date\"],\n",
    "            \"severity_scale\": line[\"Echelle\"],\n",
    "            \"raw_data\": \"\", #line,\n",
    "            \"created_at\": \"date.now()\",\n",
    "            \"updated_at\": \"\",\n",
    "        }\n",
    "\n",
    "        causes = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"event_category\": line[\"Causes profondes\"],\n",
    "            \"equipment_failure\": line[\"Causes premières\"],\n",
    "            \"description\": line[\"Contenu\"], # could also reuse Contenu\n",
    "        }\n",
    "\n",
    "        substances = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"name\": line[\"Matières\"],\n",
    "            \"cas_number\": \"\",\n",
    "            \"quantity\": \"\",\n",
    "            \"clp_class\" : line[\"Classe de danger CLP\"]\n",
    "        }\n",
    "\n",
    "        consequences_human = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"fatalities\": None,\n",
    "            \"injuries\": None,\n",
    "            \"evacuated\": None,\n",
    "            \"hospitalized\": None,\n",
    "        }\n",
    "\n",
    "        consequences = {\n",
    "            \"ENVIRONNEMENTALES\" : \"\",\n",
    "            \"ÉCONOMIQUES\" : \"\"\n",
    "        }\n",
    "\n",
    "        try :\n",
    "            for consequence in line[\"Conséquences\"].split(\"CONSÉQUENCES \"):\n",
    "                if len(consequence) < 2 : continue\n",
    "                s = consequence.split(',')\n",
    "                key = s[0]\n",
    "                content = (','.join(s[1:])).removesuffix(',')\n",
    "                consequences[key] = content\n",
    "        except : pass\n",
    "\n",
    "        consequences_other = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"environmental_impact\": consequences[\"ENVIRONNEMENTALES\"],\n",
    "            \"economic_cost\": consequences[\"ÉCONOMIQUES\"],\n",
    "            \"disruption_duration\": line[\"Type évènement\"]\n",
    "        }\n",
    "\n",
    "        tables = {\n",
    "            \"sites\": sites,\n",
    "            \"accidents\": accidents,\n",
    "            \"causes\": causes,\n",
    "            \"substances\": substances,\n",
    "            \"consequences_human\": consequences_human,\n",
    "            \"consequences_other\": consequences_other\n",
    "        }\n",
    "\n",
    "        # taken_cols = [\"Titre\", \"Pays\", \"Code NAF\", \"Numéro ARIA\", \"Date\", \"Echelle\", \"Causes profondes\", \"Causes premières\", \"Contenu\", \"Matières\", \"Conséquences\", \"Départment\", \"Commune\", \"Classe de danger CLP\", \"Type évènement\"]\n",
    "        # print(line.drop(labels=taken_cols, errors='ignore'))\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    db_lines = []\n",
    "\n",
    "    for x in tqdm(iter(df.iloc), total=trunc, ncols=200):\n",
    "        db_lines.append(create_line(x))\n",
    "\n",
    "    return db_lines\n",
    "\n",
    "ARIA_db_jsons = convert_to_db(df, trunc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON, display\n",
    "\n",
    "def print_db_jsons(json):\n",
    "    for i, db_line in enumerate(json):\n",
    "        print(i, \"=\" * 200)\n",
    "        for key in db_line :\n",
    "            print(key, flush=True, end='')\n",
    "            display(JSON(db_line[key], expanded=True))\n",
    "        if i == 0 : break\n",
    "\n",
    "print_db_jsons(ARIA_db_jsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0156a",
   "metadata": {},
   "source": [
    "# OSHA - Injuries (ITA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5110c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./ITA Case Detail Data 2024 through 08-31-2025.csv\", sep=\",\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bac5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3dbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_db(df : pd.DataFrame, trunc = None):    \n",
    "\n",
    "    if trunc is not None :\n",
    "        df = df.head(trunc)\n",
    "\n",
    "    def create_line(line : pd.Series):\n",
    "        address = f\"{line['street_address']} {line['city']} {line['state']} {line['zip_code']}\"\n",
    "        site_key = line[\"establishment_name\"] + \" \" + address\n",
    "        site_id = str(uuid5(UUID_NAMESPACE, site_key))\n",
    "        sites = {\n",
    "            \"site_id\" : site_id,\n",
    "            \"plant_name\": line[\"establishment_name\"],  # or fallback to \"company_name\"\n",
    "            \"address\": address,\n",
    "            \"latitude\": None,  # Geocode later from address\n",
    "            \"longitude\": None,\n",
    "            \"country\": \"USA\",\n",
    "            \"industrial_activity\": str(line[\"naics_code\"]),  # Maps to NAF equivalent\n",
    "        }\n",
    "\n",
    "        accident_key = \" \".join([line[\"job_description\"], line[\"date_of_incident\"]])\n",
    "        accident_id = str(uuid5(UUID_NAMESPACE, accident_key))\n",
    "        accidents = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"site_id\": site_id,\n",
    "            \"title\": line[\"job_description\"],  # Brief incident summary\n",
    "            \"source\": \"OSHA ITA\",\n",
    "            \"source_id\": line[\"case_number\"],  # Unique OSHA case identifier\n",
    "            \"accident_date\": line[\"date_of_incident\"],\n",
    "            \"severity_scale\": int(line[\"incident_outcome\"]),  # 1 = Death / 2 = Days away from work / 3 = Job transfer or restriction / 4 = Other recordable case\n",
    "            \"raw_data\": \"\",# line.to_dict(),  # Full line as JSON\n",
    "            \"created_at\": \"date.now()\",\n",
    "            \"updated_at\": \"\",  # Fill on save\n",
    "        }\n",
    "\n",
    "        causes = {\n",
    "            \"accident_id\": accident_id, \n",
    "            \"event_category\": line[\"NEW_NAR_WHAT_HAPPENED\"],  # Deep/root causes\n",
    "            \"equipment_failure\": line[\"NEW_NAR_BEFORE_INCIDENT\"],  # Initial triggers\n",
    "            \"description\": line[\"NEW_INCIDENT_DESCRIPTION\"],\n",
    "        }\n",
    "\n",
    "        substances = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"name\": line[\"NEW_NAR_OBJECT_SUBSTANCE\"],  # Object/substance hit/contacted\n",
    "            \"cas_number\": \"\",  # Not in ITA; research via name if needed\n",
    "            \"quantity\": \"\",  # Derive from context if available\n",
    "            \"clp_class\": line[\"NEW_NAR_INJURY_ILLNESS\"],  # Injury type as hazard proxy\n",
    "        }\n",
    "\n",
    "        consequences_human = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"fatalities\": 1 if pd.notna(line[\"date_of_death\"]) else 0,\n",
    "            \"injuries\": 1,  # Each line is one recordable case\n",
    "            \"evacuated\": None,  # Not directly available\n",
    "            \"hospitalized\": 1 if line[\"dafw_num_away\"] > 0 else 0,  # Days away implies severity\n",
    "        }\n",
    "\n",
    "        consequences_other = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"environmental_impact\": \"\",  # ITA focuses on worker injuries\n",
    "            \"economic_cost\": \"\",  # Estimate from total_hours_worked if needed\n",
    "            \"disruption_duration\": int(line[\"djtr_num_tr\"]),  # Restriction days as proxy\n",
    "        }\n",
    "\n",
    "        tables = {\n",
    "            \"sites\": sites,\n",
    "            \"accidents\": accidents,\n",
    "            \"causes\": causes,\n",
    "            \"substances\": substances,\n",
    "            \"consequences_human\": consequences_human,\n",
    "            \"consequences_other\": consequences_other\n",
    "        }\n",
    "\n",
    "        # taken_cols = [\"Titre\", \"Pays\", \"Code NAF\", \"Numéro ARIA\", \"Date\", \"Echelle\", \"Causes profondes\", \"Causes premières\", \"Contenu\", \"Matières\", \"Conséquences\", \"Départment\", \"Commune\", \"Classe de danger CLP\", \"Type évènement\"]\n",
    "        # print(line.drop(labels=taken_cols, errors='ignore'))\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    db_lines = []\n",
    "\n",
    "    if trunc is None:\n",
    "        itr = iter(df.iloc)\n",
    "    else :\n",
    "        itr = iter(df.head(trunc).iloc)\n",
    "\n",
    "    for x in tqdm(itr, total=5, ncols=200):\n",
    "        db_lines.append(create_line(x))\n",
    "\n",
    "    return db_lines\n",
    "\n",
    "# OSHA_db_jsons = convert_to_db(df, trunc=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_db_jsons(OSHA_db_jsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6272fc4",
   "metadata": {},
   "source": [
    "# EPICEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "epicea_example = {\n",
    "    \"Numéro du dossier\": \"27615\",\n",
    "    \"Comité technique national\": \"B - Bâtiment et Travaux Publics\",\n",
    "    \"Code entreprise\": \"4321A - Travaux d'installation électrique dans tous locaux\",\n",
    "    \"Matériel en cause\": \"270302 - Chaudière à mazout\",\n",
    "    \"Résumé de l'accident\": r\"Une équipe de trois salariés dont un plombier chauffagiste, âgé de 30 ans, intervient chez un particulier, un vétérinaire dont l'habitation se compose d'une maison et d'un bloc opératoire (achetée il y a 21 ans auparavant, maison construite dans les années 70). L'installation de chauffage de l'habitation, composée d’une pompe à chaleur et d’une chaudière au fioul, alimente les 28 radiateurs de la propriété. L'alimentation en eau chaude est assurée par un ballon d'eau indépendant de l'installation de chauffage. Une chaudière au fioul de la marque *** est placée dans un local à proximité de l'habitation. La chaudière fonctionne en relai de la pompe à chaleur. Quand la pompe n'arrive plus à maintenir la consigne de température, la chaudière au fioul fonctionne (le propriétaire de l’installation ne dispose pas du manuel d'utilisation ou d'installation de la chaudière). Le propriétaire rencontre des difficultés sur le fonctionnement de la chaudière au fioul mais ne valide pas le premier devis pour la remplacer. Le second devis propose une remise en état de la chaudière existante avec un désembouage des radiateurs, le remplacement des vannes des radiateurs, l'alimentation entre la cuve de fioul et la chaudière et le ramonage. Ce devis est validé par le propriétaire. Le jour de l’accident, l’équipe intervient pour cette prestation approuvée. Le plombier chauffagiste réalise seul l'entretien de la chaudière dans le local dédié et ses deux collègues réalisent le désembouage des radiateurs dans l’habitation. Pour réaliser le test de combustion, le plombier chauffagiste allume la chaudière. Il met la sonde de l'analyseur de combustion au niveau de l'évacuation des fumées. Suivant le brûleur et les valeurs fournies par l'analyseur, il règle la chaudière. Il est accroupi à proximité de la chaudière. La chaudière explose, le local de la chaudière est envahi de fumée et de vapeurs d'eau. A la suite de l’explosion, le brûleur de la chaudière est au sol devant la chaudière. Il a été éjecté. La trappe d'accès de la chambre de combustion, au-dessus du brûleur, est cassée sur un angle. A l'intérieur de la chambre de combustion, dans le fond, la cuve en fonte présente un trou. Le plombier chauffagiste se dirige vers la sortie, puis vers le véhicule de l'entreprise. Il se regarde dans le rétroviseur du véhicule car son visage le brûle. Il constate qu'il n'est pas brûlé sur le visage. Il se déshabille lui-même. Son collègue appelle les pompiers. A leur arrivée, le plombier est dans la baignoire de l'habitation à une eau tempérée, toujours conscient. Il est brûlé par de la vapeur d’eau sur plus de 50 % du corps aux niveaux des bras, du torse, des parties intimes et des jambes. Hypothèses pour expliquer l’explosion de la chaudière : sur le départ du réseau d'eau de chauffage de la chaudière, il y a des dispositifs de sécurité : un vase d'expansion suivi d'une soupape de sécurité. Le vase d'expansion compense le surplus de volume d'eau quand elle chauffe, il se déclenche quand la température atteint 60°C. La soupape de sécurité se déclenche si la pression atteint les 3 bars. La vanne purge l'eau du circuit pour qu'elle s'écoule dans le local de la chaudière, hors du réseau d'eau de chauffage. Le jour de l'accident, la vanne d’isolement (entre la chaudière et le réseau d’eau de l’habitation pour le chauffage) est fermée. Ce qui permet au plombier chauffagiste de réaliser le contrôle de combustion sur la chaudière et à ses collègues de continuer le désembouage sur les radiateurs. Cette vanne est située avant les dispositifs de sécurité de la chaudière. De ce fait, lorsque le plombier chauffagiste a allumé la chaudière, la température de l'eau augmente ce qui engendre une élévation de la pression. Les dispositifs de sécurité étant situés après la vanne d'isolement, ils ne se déclenchent pas lorsque la température et la pression atteignent les valeurs de déclenchement. La chaudière continue de chauffer l’eau jusqu’à ce qu’elle se transforme en vapeur. La pression dans le circuit de la chaudière est telle que la chaudière explose avec la libération de la vapeur d'eau dans le local.\"\n",
    "}\n",
    "\n",
    "len(epicea_example[\"Résumé de l'accident\"].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edcc6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)  # Should show +cu130\n",
    "print(torch.version.cuda)  # Should show 12.1\n",
    "print(torch.cuda.is_available())  # True\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Step 1: Tokenizer (Qwen requires trust_remote_code)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Step 2: Proper pad token setup\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Step 3: Clear GPU memory\n",
    "try : \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "except : pass\n",
    "gc.collect()\n",
    "\n",
    "# Step 4: Model loading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Step 5: Move to GPU manually\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# Step 6: Pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Qwen2.5-1.5B-Instruct loaded successfully on GPU!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ai(field: str, context: str) -> str:   \n",
    "    # French-only SYSTEM prompt\n",
    "    system_msg = {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"Tu es un assistant français. Réponds UNIQUEMENT en français avec des réponses courtes et précises. Ne jamais utiliser l'anglais.\"\n",
    "    }\n",
    "    \n",
    "    prompts = {\n",
    "        \"title\" : f\"J'ai besoin d'un titre, résume en une petite phrase cette description:\\n{context}\",\n",
    "        \"severity_scale\" : f\"Par rapport au standard SEVESO, note la gravité de la description suivante de 1 à 5:\\n{context} \\n\\n\\nNe répond qu'un seul chiffre. Si tu n'es pas sur, réponds 1\",\n",
    "        \"fatalities\" : f\"Combien de morts y a t il dans la description suivante:\\n{context} \\n\\n\\nNe répond qu'un seul nombre\",\n",
    "        \"injuries\" : f\"Combien de blessés y a t il dans la description suivante:\\n{context} \\n\\n\\nNe répond qu'un seul nombre\",\n",
    "        \"evacuated\" : f\"Combien de personnes évacuées y a t il dans la description suivante:\\n{context} \\n\\n\\nNe répond qu'un seul nombre\",\n",
    "        \"hospitalized\" : f\"Combien de personnes hospitalisées y a t il dans la description suivante:\\n{context} \\n\\n\\nNe répond qu'un seul nombre\",\n",
    "    }\n",
    "\n",
    "    # \"environmental_impact\":\"<AI> To Prompt\",\"economic_cost\":\"<AI> To Prompt\",\"disruption_duration\":\"<AI> To Prompt\"\n",
    "\n",
    "    if field not in prompts.keys():\n",
    "        return \"<AI> To Prompt\"\n",
    "\n",
    "    user_msg = {\"role\": \"user\", \"content\": prompts.get(field, f\"Retrouve l'information \\\"{field}\\\"\\ndans le texte suivant:{context}\")}\n",
    "    \n",
    "    # Qwen chat template\n",
    "    messages = [system_msg, user_msg]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    result = generator(\n",
    "        prompt, \n",
    "        max_new_tokens=300,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.3,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    \n",
    "    generated = result[0]['generated_text']\n",
    "    response = generated.split('<|im_end|>\\n<|im_start|>assistant\\n')[-1].strip().strip('.').strip()\n",
    "\n",
    "    print(field, response)\n",
    "    return f\"<AI> {response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_db(df : pd.DataFrame, trunc = None):    \n",
    "\n",
    "    if trunc is not None :\n",
    "        df = df.head(trunc)\n",
    "\n",
    "    def create_line(line : pd.Series):\n",
    "        address = \"NULL\"\n",
    "        site_id = str(uuid5(UUID_NAMESPACE, address))\n",
    "        sites = {\n",
    "            \"site_id\" : site_id,\n",
    "            \"plant_name\": \"\",\n",
    "            \"address\": address,\n",
    "            \"latitude\": None,               # to fill later\n",
    "            \"longitude\": None,              # to fill later\n",
    "            \"country\": \"France\",\n",
    "            \"industrial_activity\": line[\"Comité technique national\"].split(' - ')[1],\n",
    "        }\n",
    "\n",
    "        CONTEXT_FOR_AI = line[\"Résumé de l'accident\"]\n",
    "        title = ask_ai(\"title\", CONTEXT_FOR_AI)\n",
    "\n",
    "        accident_key = \" \".join([title, str(line[\"Numéro du dossier\"])])\n",
    "        accident_id = str(uuid5(UUID_NAMESPACE, accident_key))\n",
    "        accidents = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"site_id\": site_id,\n",
    "            \"title\": title,\n",
    "            \"source\": \"EPICEA\",\n",
    "            \"source_id\": str(line[\"Numéro du dossier\"]),\n",
    "            \"accident_date\": \"NULL\",\n",
    "            \"severity_scale\": ask_ai(\"severity_scale\", CONTEXT_FOR_AI),\n",
    "            \"raw_data\": \"\", #line,\n",
    "            \"created_at\": \"date.now()\",\n",
    "            \"updated_at\": \"\",\n",
    "        }\n",
    "\n",
    "        causes = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"event_category\": line[\"Code entreprise\"].split(' - ')[1],\n",
    "            \"equipment_failure\": line[\"Matériel en cause\"],\n",
    "            \"description\": line[\"Résumé de l'accident\"], \n",
    "        }\n",
    "\n",
    "        substances = {\"substance\":ask_ai(\"substances\", CONTEXT_FOR_AI)}\n",
    "\n",
    "        # substances = {\n",
    "        #     \"accident_id\": accident_id,\n",
    "        #     \"name\": \"\",\n",
    "        #     \"cas_number\": \"\",\n",
    "        #     \"quantity\": \"\",\n",
    "        #     \"clp_class\" : \"\"\n",
    "        # }\n",
    "\n",
    "        consequences_human = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"fatalities\": ask_ai(\"fatalities\", CONTEXT_FOR_AI),\n",
    "            \"injuries\": ask_ai(\"injuries\", CONTEXT_FOR_AI),\n",
    "            \"evacuated\": ask_ai(\"evacuated\", CONTEXT_FOR_AI),\n",
    "            \"hospitalized\": ask_ai(\"hospitalized\", CONTEXT_FOR_AI),\n",
    "        }\n",
    "\n",
    "        consequences_other = {\n",
    "            \"accident_id\": accident_id,\n",
    "            \"environmental_impact\": ask_ai(\"environmental_impact\", CONTEXT_FOR_AI),\n",
    "            \"economic_cost\": ask_ai(\"economic_cost\", CONTEXT_FOR_AI),\n",
    "            \"disruption_duration\": ask_ai(\"disruption_duration\", CONTEXT_FOR_AI)\n",
    "        }\n",
    "\n",
    "        tables = {\n",
    "            \"sites\": sites,\n",
    "            \"accidents\": accidents,\n",
    "            \"causes\": causes,\n",
    "            \"substances\": substances,\n",
    "            \"consequences_human\": consequences_human,\n",
    "            \"consequences_other\": consequences_other\n",
    "        }\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    db_lines = []\n",
    "\n",
    "    for x in tqdm(iter(df.iloc), total=trunc, ncols=200):\n",
    "        db_lines.append(create_line(x))\n",
    "\n",
    "    return db_lines\n",
    "\n",
    "df = pd.DataFrame(epicea_example, index=[0])\n",
    "EPICEA_db_jsons = convert_to_db(df)\n",
    "print(type(EPICEA_db_jsons))\n",
    "print_db_jsons(EPICEA_db_jsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e77b41",
   "metadata": {},
   "source": [
    "# Database Inserting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d644c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def insert_jsons_in_db(db_jsons, conn):\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # 1. Insert sites\n",
    "    # Generate an array of tuples without duplicates\n",
    "    constraint_set = set()\n",
    "    sites_tuples = []\n",
    "    for db_json in db_jsons:\n",
    "        constraint_key = db_json[\"sites\"][\"plant_name\"] + db_json[\"sites\"][\"address\"]\n",
    "        if constraint_key in constraint_set : continue\n",
    "        constraint_set.add(constraint_key)\n",
    "\n",
    "        sites_tuples.append((\n",
    "            db_json[\"sites\"][\"site_id\"],\n",
    "            db_json[\"sites\"][\"plant_name\"], \n",
    "            db_json[\"sites\"][\"address\"], \n",
    "            db_json[\"sites\"][\"latitude\"], \n",
    "            db_json[\"sites\"][\"longitude\"], \n",
    "            db_json[\"sites\"][\"country\"], \n",
    "            db_json[\"sites\"][\"industrial_activity\"]\n",
    "        ))\n",
    "\n",
    "    print(\"Inserting sites\")\n",
    "    execute_values(cur, \"\"\"INSERT INTO sites (site_id, plant_name, address, latitude, longitude, country, industrial_activity) VALUES %s ON CONFLICT (plant_name, address) DO NOTHING\"\"\", sites_tuples)\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT site_id, plant_name, address \n",
    "        FROM sites\n",
    "    \"\"\")\n",
    "    all_sites = cur.fetchall()\n",
    "    site_mapping = {(row[1], row[2]): row[0] for row in all_sites}\n",
    "\n",
    "    # Insert accidents  \n",
    "    accidents_tuples = [\n",
    "        (\n",
    "            db_json[\"accidents\"][\"accident_id\"], \n",
    "            site_mapping[(db_json[\"sites\"][\"plant_name\"], db_json[\"sites\"][\"address\"])], \n",
    "            db_json[\"accidents\"][\"title\"], \n",
    "            db_json[\"accidents\"][\"source\"], \n",
    "            db_json[\"accidents\"][\"source_id\"], \n",
    "            db_json[\"accidents\"][\"accident_date\"], \n",
    "            db_json[\"accidents\"][\"severity_scale\"]\n",
    "        ) \n",
    "        for db_json in db_jsons\n",
    "    ]\n",
    "    print(\"Inserting accidents\")\n",
    "    execute_values(cur, \"\"\"INSERT INTO accidents (accident_id, site_id, title, source, source_id, accident_date, severity_scale) VALUES %s ON CONFLICT DO NOTHING\"\"\", accidents_tuples)\n",
    "\n",
    "    # Insert causes\n",
    "    causes_tuples = [\n",
    "        (\n",
    "            db_json[\"causes\"][\"accident_id\"], \n",
    "            db_json[\"causes\"][\"event_category\"], \n",
    "            db_json[\"causes\"][\"equipment_failure\"], \n",
    "            db_json[\"causes\"][\"description\"]\n",
    "        ) \n",
    "        for db_json in db_jsons\n",
    "    ]\n",
    "    print(\"Inserting causes\")\n",
    "    execute_values(cur, \"\"\"INSERT INTO causes (accident_id, event_category, equipment_failure, description) VALUES %s ON CONFLICT (accident_id) DO NOTHING\"\"\", causes_tuples)\n",
    "\n",
    "    # Insert substances\n",
    "    substances_tuples = [\n",
    "        (\n",
    "            db_json[\"substances\"][\"accident_id\"], \n",
    "            db_json[\"substances\"][\"name\"], \n",
    "            db_json[\"substances\"][\"cas_number\"], \n",
    "            db_json[\"substances\"][\"quantity\"], \n",
    "            db_json[\"substances\"][\"clp_class\"]\n",
    "        ) \n",
    "        for db_json in db_jsons\n",
    "    ]\n",
    "    print(\"Inserting substances\")\n",
    "    execute_values(cur, \"\"\"INSERT INTO substances (accident_id, name, cas_number, quantity, clp_class) VALUES %s ON CONFLICT (accident_id) DO NOTHING\"\"\", substances_tuples)\n",
    "\n",
    "    # Insert human consequences\n",
    "    human_tuples = [\n",
    "        (\n",
    "            db_json[\"consequences_human\"][\"accident_id\"], \n",
    "            db_json[\"consequences_human\"][\"fatalities\"], \n",
    "            db_json[\"consequences_human\"][\"injuries\"], \n",
    "            db_json[\"consequences_human\"][\"evacuated\"], \n",
    "            db_json[\"consequences_human\"][\"hospitalized\"]\n",
    "        ) \n",
    "        for db_json in db_jsons\n",
    "    ]\n",
    "    print(\"Inserting consequences_human\")\n",
    "    execute_values(cur, \"\"\"INSERT INTO consequences_human (accident_id, fatalities, injuries, evacuated, hospitalized) VALUES %s ON CONFLICT (accident_id) DO NOTHING\"\"\", human_tuples)\n",
    "\n",
    "    # Insert other consequences\n",
    "    other_tuples = [\n",
    "        (\n",
    "            db_json[\"consequences_other\"][\"accident_id\"], \n",
    "            db_json[\"consequences_other\"][\"environmental_impact\"], \n",
    "            db_json[\"consequences_other\"][\"economic_cost\"], \n",
    "            db_json[\"consequences_other\"][\"disruption_duration\"]\n",
    "        ) \n",
    "        for db_json in db_jsons\n",
    "    ]\n",
    "    print(\"Inserting consequences_other\")\n",
    "    execute_values(cur, \"\"\"INSERT INTO consequences_other (accident_id, environmental_impact, economic_cost, disruption_duration) VALUES %s ON CONFLICT (accident_id) DO NOTHING\"\"\", other_tuples)\n",
    "\n",
    "    # Commit all inserts\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "print(len(ARIA_db_jsons))\n",
    "\n",
    "with psycopg2.connect(host=\"localhost\", database=\"postgres\", user=\"postgres\", password=\"7833\", port=5432) as conn : \n",
    "    insert_jsons_in_db(ARIA_db_jsons, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb69ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
